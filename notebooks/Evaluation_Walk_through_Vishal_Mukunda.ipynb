{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One run full walktrhough "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the full walk through on the large data set\n",
    "* Refactor the source code and bring it to individual scripts\n",
    "* Ensure a full run with one click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM\n",
    "\n",
    "CRISP-DM stands for cross-industry process for data mining. The CRISP-DM methodology provides a structured approach to planning a data mining project.\n",
    "\n",
    "![CRISP-DM.JPG](../reports/figures/CRISP-DM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "#### The scope of this project is to trace Corona Virus spread across the countries. \n",
    "* To trace the confirmed cases for all the countries\n",
    "* To calculate the doubling rate.\n",
    "* To simulate the spread of COVID-19 in Brazil using SIR model and also for 100+ countries.\n",
    "* We would like to understand the data quality \n",
    "* Everything should be automated.\n",
    "* To create a user friendly dashboard, which shows the current count of confirmed cases, doubling rate and SIR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your base path is at: ads_covid-19'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check some parameters\n",
    "## depending where you launch your notebook, the relative path might not work\n",
    "## you should start the notebook server from your base path\n",
    "## when opening the notebook, typically your path will be ../ads_covid-19/notebooks\n",
    "import os\n",
    "if os.path.split(os.getcwd())[-1]=='notebooks':\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "'Your base path is at: '+os.path.split(os.getcwd())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update all data (Data Understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Exists: Fetch the latest data from repository\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_johns_hopkins():\n",
    "    ''' Get data by a git pull request, the source code has to be pulled first\n",
    "        Result is stored in the predifined csv structure. If there is no Repository \n",
    "        not present then clone the data from GitHub.\n",
    "    '''\n",
    "    \n",
    "if os.path.exists('data/raw/COVID-19/'):\n",
    "    print('Repository Exists: Fetch the latest data from repository')\n",
    "    git_pull = subprocess.Popen( \"git pull\" ,\n",
    "                         cwd = os.path.dirname('data/raw/COVID-19/' ),\n",
    "                         shell = True,\n",
    "                         stdout = subprocess.PIPE,\n",
    "                         stderr = subprocess.PIPE )\n",
    "    (out, error) = git_pull.communicate()\n",
    "else:\n",
    "    print('Repository not present. Fetch the entire repository')\n",
    "    git_clone = subprocess.Popen( \"git clone https://github.com/CSSEGISandData/COVID-19.git\" ,\n",
    "                         cwd = os.path.dirname('data/raw/' ),\n",
    "                         shell = True,\n",
    "                         stdout = subprocess.PIPE,\n",
    "                         stderr = subprocess.PIPE )\n",
    "    (out, error) = git_clone.communicate()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_johns_hopkins()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process pipeline (Data Preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows stored: 59724\n",
      " Latest date is: 2020-09-14 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def store_relational_data():\n",
    "    ''' Transformes the COVID data in a relational data set\n",
    "\n",
    "    '''\n",
    "    data_path='data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "    pd_raw=pd.read_csv(data_path)\n",
    "\n",
    "    time_idx = pd_raw.columns[4:]\n",
    "    df_plot = pd.DataFrame({\n",
    "        'date':time_idx})\n",
    "    df_input_large= pd_raw['Country/Region'].unique()\n",
    "    \n",
    "    for each in df_input_large:\n",
    "        df_plot[each] =np.array(pd_raw[pd_raw['Country/Region']==each].iloc[:,4::].sum(axis=0))\n",
    "    df = df_plot.drop('date', axis=1)\n",
    "    \n",
    "    #Merging the data set over COUNTRY for CODE column for worldmap\n",
    "    df_code = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv')\n",
    "    world_raw =  pd.DataFrame({\"COUNTRY\" : df_input_large, \"Confirm cases\" :df.iloc[-1]})\n",
    "    world_con = pd.merge(world_raw, df_code, on = \"COUNTRY\").drop('GDP (BILLIONS)', axis=1)\n",
    "    world_con.to_csv('data/processed/COVID_CRD.csv',sep=';',index=False)\n",
    "    \n",
    "    #Continuation of data preparation \n",
    "    pd_data_base=pd_raw.rename(columns={'Country/Region':'COUNTRY',\n",
    "                      'Province/State':'state'})\n",
    "\n",
    "    pd_data_base['state']=pd_data_base['state'].fillna('no')\n",
    "\n",
    "    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)\n",
    "\n",
    "\n",
    "    pd_relational_model_1=pd_data_base.set_index(['state','COUNTRY']) \\\n",
    "                                .T                              \\\n",
    "                                .stack(level=[0,1])             \\\n",
    "                                .reset_index()                  \\\n",
    "                                .rename(columns={'level_0':'date',\n",
    "                                                   0:'confirmed'},\n",
    "                                                  )\n",
    "    pd_relational_model = pd.merge(pd_relational_model_1, df_code, on = \"COUNTRY\").drop('GDP (BILLIONS)', axis=1)\n",
    "    pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')\n",
    "    \n",
    "    pd_relational_model.to_csv('data/processed/20200823_COVID_relational_confirmed.csv',sep=';',index=False)\n",
    "    \n",
    "    \n",
    "    #SIR model data preparation\n",
    "    sir_plot = pd.DataFrame({\n",
    "    'date':time_idx})\n",
    "    #sir_plot.head()\n",
    "    sir_arr= pd_raw['Country/Region'].unique()\n",
    "    sir_list = sir_arr.tolist()\n",
    "    for each in sir_list:\n",
    "        sir_plot[each] =np.array(pd_raw[pd_raw['Country/Region']==each].iloc[:,4::].sum(axis=0))\n",
    "    #sir_plot.head()\n",
    "    \n",
    "    #Creating SIR plot for 100+ countries\n",
    "    sir_plot= sir_plot.drop(columns = ['Taiwan*', 'South Sudan', 'Guyana','Haiti', 'Holy See', 'Honduras', 'Hungary', 'Iceland',\n",
    "                                   'Iraq', 'Ireland', 'Israel', 'Italy',\n",
    "       'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya',\n",
    "       'Korea, South', 'Kosovo','Belgium', 'Belize', 'Benin', 'Bhutan',\n",
    "       'Bolivia', 'Bosnia and Herzegovina', 'Botswana',\n",
    "       'Brunei', 'Bulgaria', 'Burkina Faso', 'Burma', 'Burundi',\n",
    "       'Cabo Verde', 'Cambodia', 'Cameroon', 'Canada',\n",
    "       'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia',\n",
    "       'Comoros', 'Congo (Brazzaville)', 'Congo (Kinshasa)', 'Costa Rica',\n",
    "       \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Cyprus', 'Czechia', 'Denmark',\n",
    "       'Diamond Princess', 'Djibouti', 'Luxembourg', 'MS Zaandam', 'Madagascar', 'Malawi',\n",
    "       'Malaysia', 'Maldives', 'Mali', 'Malta', 'Mauritania', 'Mauritius',\n",
    "       'Mexico', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco',\n",
    "       'Mozambique', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand',\n",
    "       'Nicaragua', 'Niger', 'Panama', 'Papua New Guinea', 'Paraguay',\n",
    "       'Peru', 'Philippines', 'Bahamas', 'Egypt'])\n",
    "    time_idx = [datetime.strptime(each, \"%m/%d/%y\") for each in sir_plot.date] #to convert all the dates into datetime \n",
    "    time_str= [each.strftime('%Y-%m-%d') for each in time_idx] #to convert datetime function to string\n",
    "    #time_str[0:5]\n",
    "    \n",
    "    #Storing the processed data file and sep';' is a seperator [German std]\n",
    "    sir_plot.to_csv('C:/Users/Vishal Sharbidar/Desktop/ADS_COVID-19/ads_covid-19/data/processed/COVID_sir_flat_table.csv', sep=';',index=False)\n",
    "    \n",
    "    \n",
    "    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))\n",
    "    print(' Latest date is: '+str(max(pd_relational_model.date)))\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    store_relational_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Doubling Rate Calculation (Modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test slope is: [2.]\n",
      "            date state  COUNTRY  confirmed CODE  confirmed_filtered  \\\n",
      "32701 2020-09-10    no  Germany   258149.0  DEU            258018.2   \n",
      "32702 2020-09-11    no  Germany   259735.0  DEU            259374.2   \n",
      "32703 2020-09-12    no  Germany   260817.0  DEU            260732.0   \n",
      "32704 2020-09-13    no  Germany   261737.0  DEU            261946.8   \n",
      "32705 2020-09-14    no  Germany   263222.0  DEU            263161.6   \n",
      "\n",
      "       confirmed_DR  confirmed_filtered_DR  \n",
      "32701    160.722431             168.789051  \n",
      "32702    156.332930             184.661656  \n",
      "32703    194.577961             191.152480  \n",
      "32704    260.502498             202.662158  \n",
      "32705    217.817325             215.629569  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "def calc_doubling_rate(df_input,filter_on='confirmed'):\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','COUNTRY',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "\n",
    "    pd_DR_result= df_input.groupby(['state','COUNTRY']).apply(rolling_reg,filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result=pd_DR_result.rename(columns={filter_on:filter_on+'_DR',\n",
    "                             'level_2':'index'})\n",
    "\n",
    "    #we do the merge on the index of our big table and on the index column after groupby\n",
    "    df_output=pd.merge(df_input,pd_DR_result[['index',str(filter_on+'_DR')]],left_index=True,right_on=['index'],how='left')\n",
    "    df_output=df_output.drop(columns=['index'])\n",
    "\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def get_doubling_time_via_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Doubling rate: double\n",
    "    '''\n",
    "\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1,2).reshape(-1, 1)\n",
    "\n",
    "    assert len(in_array)==3\n",
    "    reg.fit(X,y)\n",
    "    intercept=reg.intercept_\n",
    "    slope=reg.coef_\n",
    "\n",
    "    return intercept/slope\n",
    "\n",
    "\n",
    "def savgol_filter(df_input,column='confirmed',window=5):\n",
    "    ''' Savgol Filter is a digital filter that can be applied to a set of digital data points for the purpose of \n",
    "        smoothing the data, that is, to increase the precision of the data without distorting the signal tendency.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the df_input has to be preserved in result\n",
    "    '''\n",
    "\n",
    "    degree=1\n",
    "    df_result=df_input\n",
    "\n",
    "    filter_in=df_input[column].fillna(0) # attention with the neutral element here\n",
    "\n",
    "    result=signal.savgol_filter(np.array(filter_in),\n",
    "                           window, # window size used for filtering\n",
    "                           1)\n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "def rolling_reg(df_input,col='confirmed'):\n",
    "    ''' Rolling Regression is used to approximate the doubling time'\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(\n",
    "                window=days_back,\n",
    "                min_periods=days_back).apply(get_doubling_time_via_regression,raw=False)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','COUNTRY',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    df_output=df_input.copy() # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result=df_output[['state','COUNTRY',filter_on]].groupby(['state','COUNTRY']).apply(savgol_filter)#.reset_index()\n",
    "\n",
    "    #print('--+++ after group by apply')\n",
    "    #print(pd_filtered_result[pd_filtered_result['country']=='Germany'].tail())\n",
    "\n",
    "    #df_output=pd.merge(df_output,pd_filtered_result[['index',str(filter_on+'_filtered')]],on=['index'],how='left')\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "    #print(df_output[df_output['country']=='Germany'].tail())\n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data_reg=np.array([2,4,6])\n",
    "    result=get_doubling_time_via_regression(test_data_reg)\n",
    "    print('the test slope is: '+str(result))\n",
    "\n",
    "    pd_JH_data=pd.read_csv('data/processed/20200823_COVID_relational_confirmed.csv',sep=';',parse_dates=[0])\n",
    "    pd_JH_data=pd_JH_data.sort_values('date',ascending=True).copy()\n",
    "\n",
    "    #test_structure=pd_JH_data[((pd_JH_data['country']=='US')|\n",
    "    #                  (pd_JH_data['country']=='Germany'))]\n",
    "\n",
    "    pd_result_larg=calc_filtered_data(pd_JH_data)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg,'confirmed_filtered')\n",
    "\n",
    "\n",
    "    mask=pd_result_larg['confirmed']>100\n",
    "    pd_result_larg['confirmed_filtered_DR']=pd_result_larg['confirmed_filtered_DR'].where(mask, other=np.NaN)\n",
    "    pd_result_larg.to_csv('data/processed/COVID_final_set.csv',sep=';',index=False)\n",
    "    print(pd_result_larg[pd_result_larg['COUNTRY']=='Germany'].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIR Modelling \n",
    "\n",
    "Susceptible Infected and Recovered model is used to fit the Gamma(Infection rate) and Beta(recovery rate) for each country by using the available data and I have also generated the fitted curve to show how well the processed data fits over truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishal Sharbidar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\Vishal Sharbidar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\Vishal Sharbidar\\anaconda3\\lib\\site-packages\\scipy\\integrate\\odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "C:\\Users\\Vishal Sharbidar\\anaconda3\\lib\\site-packages\\scipy\\optimize\\minpack.py:829: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "from scipy import integrate\n",
    "\n",
    "df_input_large=pd.read_csv('data/processed/COVID_sir_flat_table.csv',sep=';').iloc[80:]\n",
    "pop = pd.read_csv('data/processed/population.csv',sep=';')\n",
    "\n",
    "df_all = df_input_large.columns\n",
    "df_all = list(df_all)\n",
    "\n",
    "def SIR_model(SIR,beta,gamma):\n",
    "    ''' Simple SIR model\n",
    "        S: susceptible population\n",
    "        I: infected people\n",
    "        R: recovered people\n",
    "        beta: \n",
    "        \n",
    "        overall condition is that the sum of changes (differnces) sum up to 0\n",
    "        dS+dI+dR=0\n",
    "        S+I+R= N (constant size of population)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    S,I,R=SIR\n",
    "    dS_dt=-beta*S*I/N0          #S*I is the \n",
    "    dI_dt=beta*S*I/N0-gamma*I\n",
    "    dR_dt=gamma*I\n",
    "    return([dS_dt,dI_dt,dR_dt])\n",
    "\n",
    "\n",
    "# Functions for SIR model with time step\n",
    "def SIR_model_t(SIR,t,beta,gamma):\n",
    "    ''' Simple SIR model\n",
    "        S: susceptible population\n",
    "        t: time step, mandatory for integral.odeint\n",
    "        I: infected people\n",
    "        R: recovered people\n",
    "        beta: \n",
    "        \n",
    "        overall condition is that the sum of changes (differnces) sum up to 0\n",
    "        dS+dI+dR=0\n",
    "        S+I+R= N (constant size of population)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    S,I,R=SIR\n",
    "    dS_dt=-beta*S*I/N0          #S*I is the \n",
    "    dI_dt=beta*S*I/N0-gamma*I\n",
    "    dR_dt=gamma*I\n",
    "    return dS_dt,dI_dt,dR_dt\n",
    "\n",
    "\n",
    "#Function defined for optimize curve fit\n",
    "def fit_odeint(x, beta, gamma):\n",
    "    '''\n",
    "    helper function for the integration\n",
    "    '''\n",
    "    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI\n",
    "\n",
    "#Fitting parameter for SIR model\n",
    "for each in df_all[1:]:\n",
    "    ydata = np.array(df_input_large[each])\n",
    "    t=np.arange(len(ydata))\n",
    "    N0 = 6000000 #max susceptible population\n",
    "\n",
    "    # ensure re-initialization \n",
    "    I0=ydata[0]\n",
    "    S0=N0-I0\n",
    "    R0=0\n",
    "\n",
    "    popt, pcov = optimize.curve_fit(fit_odeint, t, ydata, maxfev = 1600)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "    # get the final fitted curve\n",
    "    fitted=fit_odeint(t, *popt).reshape(-1,1)\n",
    "    df_input_large[each +'_fitted'] = fitted \n",
    "    \n",
    "df_input_large.to_csv('C:/Users/Vishal Sharbidar/Desktop/ADS_COVID-19/ads_covid-19/data/processed/COVID_sir_fitted_table.csv', sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Afghanistan</th>\n",
       "      <th>Albania</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Andorra</th>\n",
       "      <th>Angola</th>\n",
       "      <th>Antigua and Barbuda</th>\n",
       "      <th>Argentina</th>\n",
       "      <th>Armenia</th>\n",
       "      <th>Australia</th>\n",
       "      <th>...</th>\n",
       "      <th>United Kingdom_fitted</th>\n",
       "      <th>Uruguay_fitted</th>\n",
       "      <th>Uzbekistan_fitted</th>\n",
       "      <th>Venezuela_fitted</th>\n",
       "      <th>Vietnam_fitted</th>\n",
       "      <th>West Bank and Gaza_fitted</th>\n",
       "      <th>Western Sahara_fitted</th>\n",
       "      <th>Yemen_fitted</th>\n",
       "      <th>Zambia_fitted</th>\n",
       "      <th>Zimbabwe_fitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4/11/20</td>\n",
       "      <td>555</td>\n",
       "      <td>433</td>\n",
       "      <td>1825</td>\n",
       "      <td>601</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>1975</td>\n",
       "      <td>967</td>\n",
       "      <td>6303</td>\n",
       "      <td>...</td>\n",
       "      <td>90273.000000</td>\n",
       "      <td>501.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4/12/20</td>\n",
       "      <td>607</td>\n",
       "      <td>446</td>\n",
       "      <td>1914</td>\n",
       "      <td>638</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>2142</td>\n",
       "      <td>1013</td>\n",
       "      <td>6315</td>\n",
       "      <td>...</td>\n",
       "      <td>92069.117881</td>\n",
       "      <td>505.128198</td>\n",
       "      <td>793.127761</td>\n",
       "      <td>183.367051</td>\n",
       "      <td>260.302021</td>\n",
       "      <td>277.397928</td>\n",
       "      <td>4.032049</td>\n",
       "      <td>1.074632</td>\n",
       "      <td>41.920408</td>\n",
       "      <td>14.730952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4/13/20</td>\n",
       "      <td>665</td>\n",
       "      <td>467</td>\n",
       "      <td>1983</td>\n",
       "      <td>646</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>2208</td>\n",
       "      <td>1039</td>\n",
       "      <td>6351</td>\n",
       "      <td>...</td>\n",
       "      <td>93893.461146</td>\n",
       "      <td>509.290409</td>\n",
       "      <td>820.137597</td>\n",
       "      <td>192.128973</td>\n",
       "      <td>262.624581</td>\n",
       "      <td>287.124098</td>\n",
       "      <td>4.064339</td>\n",
       "      <td>1.154834</td>\n",
       "      <td>43.932859</td>\n",
       "      <td>15.500027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4/14/20</td>\n",
       "      <td>714</td>\n",
       "      <td>475</td>\n",
       "      <td>2070</td>\n",
       "      <td>659</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>2277</td>\n",
       "      <td>1067</td>\n",
       "      <td>6415</td>\n",
       "      <td>...</td>\n",
       "      <td>95746.151377</td>\n",
       "      <td>513.486913</td>\n",
       "      <td>848.058732</td>\n",
       "      <td>201.303891</td>\n",
       "      <td>264.967863</td>\n",
       "      <td>297.189881</td>\n",
       "      <td>4.096869</td>\n",
       "      <td>1.241019</td>\n",
       "      <td>46.041748</td>\n",
       "      <td>16.309207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4/15/20</td>\n",
       "      <td>784</td>\n",
       "      <td>494</td>\n",
       "      <td>2160</td>\n",
       "      <td>673</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>2443</td>\n",
       "      <td>1111</td>\n",
       "      <td>6440</td>\n",
       "      <td>...</td>\n",
       "      <td>97627.295368</td>\n",
       "      <td>517.717993</td>\n",
       "      <td>876.921322</td>\n",
       "      <td>210.910716</td>\n",
       "      <td>267.332051</td>\n",
       "      <td>307.607034</td>\n",
       "      <td>4.129643</td>\n",
       "      <td>1.333634</td>\n",
       "      <td>48.251681</td>\n",
       "      <td>17.160578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  Afghanistan  Albania  Algeria  Andorra  Angola  \\\n",
       "80  4/11/20          555      433     1825      601      19   \n",
       "81  4/12/20          607      446     1914      638      19   \n",
       "82  4/13/20          665      467     1983      646      19   \n",
       "83  4/14/20          714      475     2070      659      19   \n",
       "84  4/15/20          784      494     2160      673      19   \n",
       "\n",
       "    Antigua and Barbuda  Argentina  Armenia  Australia  ...  \\\n",
       "80                   21       1975      967       6303  ...   \n",
       "81                   21       2142     1013       6315  ...   \n",
       "82                   23       2208     1039       6351  ...   \n",
       "83                   23       2277     1067       6415  ...   \n",
       "84                   23       2443     1111       6440  ...   \n",
       "\n",
       "    United Kingdom_fitted  Uruguay_fitted  Uzbekistan_fitted  \\\n",
       "80           90273.000000      501.000000         767.000000   \n",
       "81           92069.117881      505.128198         793.127761   \n",
       "82           93893.461146      509.290409         820.137597   \n",
       "83           95746.151377      513.486913         848.058732   \n",
       "84           97627.295368      517.717993         876.921322   \n",
       "\n",
       "    Venezuela_fitted  Vietnam_fitted  West Bank and Gaza_fitted  \\\n",
       "80        175.000000      258.000000                 268.000000   \n",
       "81        183.367051      260.302021                 277.397928   \n",
       "82        192.128973      262.624581                 287.124098   \n",
       "83        201.303891      264.967863                 297.189881   \n",
       "84        210.910716      267.332051                 307.607034   \n",
       "\n",
       "    Western Sahara_fitted  Yemen_fitted  Zambia_fitted  Zimbabwe_fitted  \n",
       "80               4.000000      1.000000      40.000000        14.000000  \n",
       "81               4.032049      1.074632      41.920408        14.730952  \n",
       "82               4.064339      1.154834      43.932859        15.500027  \n",
       "83               4.096869      1.241019      46.041748        16.309207  \n",
       "84               4.129643      1.333634      48.251681        17.160578  \n",
       "\n",
       "[5 rows x 213 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input_large.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishal Sharbidar\\Desktop\\ADS_COVID-19\\ads_covid-19\n",
      "Dash is running on http://127.0.0.1:8051/\n",
      "\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "import dash\n",
    "dash.__version__\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output,State\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly import tools\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "df_input_large = pd.read_csv('data/processed/COVID_final_set.csv',sep=';')\n",
    "df = pd.read_csv('data/processed/COVID_CRD.csv',sep=';')\n",
    "df_input_sir = pd.read_csv('data/processed/COVID_sir_fitted_table.csv',sep=';')\n",
    "df_all = df_input_sir.columns\n",
    "df_all = list(df_all[:109])\n",
    "\n",
    "'''Dashboard is created by using an external stylesheet named BOOTSTRAP. \n",
    "BOOTSTRAP allows us to divide the dashboard into Rows and columns.\n",
    "COVID-19 dashbord has 5 Rows and 2 columns'''\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "app.title = 'COVID-19 Dashboard'\n",
    "\n",
    "app.layout = html.Div([\n",
    "        \n",
    "        dbc.Row(dbc.Col(html.Div(dcc.Markdown('''\n",
    "                            # Enterprise Data Science: COVID-19 Data Analytics\n",
    "                            Goals of the project:\n",
    "                            * To trace the confirmed cases for all the countries\n",
    "                            * To calculate the doubling rate.\n",
    "                            * To simulate the spread of COVID-19 in Brazil using SIR model and also for 100+ countries.\n",
    "                            * To create a user friendly dashboard, which shows the current count of confirmed cases, doubling rate and SIR model.\n",
    "                            ''')),\n",
    "                        width={'size': 10, 'offset': 1},\n",
    "                        )\n",
    "                ),\n",
    "        \n",
    "        dbc.Row(\n",
    "            [   #Dropdown for Timeline Confirmed and Doubling rate\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                            id='country_dropdown',\n",
    "                            options=[ {'label': each,'value':each} for each in df_input_large['COUNTRY'].unique()],\n",
    "                            value=['US', 'Germany','India'], # which are pre-selected\n",
    "                            multi= True),\n",
    "                        width={'size': 5, \"offset\": 0, 'order': 'first'}\n",
    "                        ),\n",
    "                #Dropdown for SIR model\n",
    "                dbc.Col(dcc.Dropdown(\n",
    "                            id='country_dropdown_sir',\n",
    "                            options=[ {'label': each,'value':each} for each in df_all[1:]],\n",
    "                            value='Brazil', # which are pre-selected\n",
    "                            multi= False\n",
    "                            ),\n",
    "                        width={'size': 5, \"offset\": 2, 'order': 'second'}\n",
    "                        ),\n",
    "                ], no_gutters=True\n",
    "        ),\n",
    "    \n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(\n",
    "                        dcc.Dropdown(\n",
    "                            id='doubling_time',\n",
    "                            options=[\n",
    "                                {'label': 'Timeline Confirmed ', 'value': 'confirmed'},\n",
    "                                {'label': 'Timeline Confirmed Filtered', 'value': 'confirmed_filtered'},\n",
    "                                {'label': 'Timeline Doubling Rate', 'value': 'confirmed_DR'},\n",
    "                                {'label': 'Timeline Doubling Rate Filtered', 'value': 'confirmed_filtered_DR'}\n",
    "                            ],\n",
    "                            value='confirmed',\n",
    "                            multi=False\n",
    "                            ),\n",
    "                        width={'size': 3, \"offset\": 0, 'order': 'first'}\n",
    "                        ),\n",
    "                \n",
    "                \n",
    "                ], \n",
    "        ),\n",
    "                \n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(dcc.Graph( \n",
    "                            id='main_window_slope'\n",
    "                            ),\n",
    "                        width=6, md={'size': 5,  \"offset\": 0, 'order': 'first'}\n",
    "                        ),\n",
    "                \n",
    "                dbc.Col(dcc.Graph(\n",
    "                            id='SIR_model'\n",
    "                            ),\n",
    "                        width=6, md={'size': 5,  \"offset\": 1, 'order': 'last'}\n",
    "                        ),\n",
    "            ]\n",
    "        ),\n",
    "        \n",
    "        dbc.Row(\n",
    "                dbc.Col(dcc.Graph(id = \"World_map\",\n",
    "                              figure = go.Figure(data = [go.Choropleth(\n",
    "                                        locations = df['CODE'],\n",
    "                                        z = df['Confirm cases'],\n",
    "                                        text = df['COUNTRY'],\n",
    "                                        colorscale = 'Blues',\n",
    "                                        autocolorscale=False,\n",
    "                                        reversescale=False,\n",
    "                                        marker_line_color='darkgray',\n",
    "                                        marker_line_width=0.5,\n",
    "                                        colorbar_title = 'Confirmed cases'\n",
    "                                        )],\n",
    "                                        layout = go.Layout(\n",
    "                                        title_text='COVID 19 WORLD MAP',\n",
    "                                        height=1300,\n",
    "                                        autosize = True,\n",
    "                                        geo=dict(\n",
    "                                            showframe=False,\n",
    "                                            showcoastlines=False,\n",
    "                                            projection_type='equirectangular'\n",
    "                                        ))\n",
    "                                     ),\n",
    "                              \n",
    "                              ),\n",
    "                        width=12, md={'size': 12,  \"offset\": 0, 'order': 'first'}\n",
    "                        ),\n",
    "         )\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('main_window_slope', 'figure'),\n",
    "    [Input('country_dropdown', 'value'),\n",
    "    Input('doubling_time', 'value')])\n",
    "\n",
    "def update_figure(country_list,show_doubling):\n",
    "\n",
    "\n",
    "    if 'DR' in show_doubling:\n",
    "        my_yaxis={'type':\"log\",\n",
    "               'title':'Approximated doubling rate over 3 days (larger numbers are better #stayathome)'\n",
    "              }\n",
    "    else:\n",
    "        my_yaxis={'type':\"log\",\n",
    "                  'title':'Confirmed infected people (source johns hopkins csse, log-scale)'\n",
    "              }\n",
    "\n",
    "\n",
    "    traces = []\n",
    "    for each in country_list:\n",
    "\n",
    "        df_plot=df_input_large[df_input_large['COUNTRY']==each]\n",
    "\n",
    "        if show_doubling=='doubling_rate_filtered':\n",
    "            df_plot=df_plot[['state','COUNTRY','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['COUNTRY','date']).agg(np.mean).reset_index()\n",
    "        else:\n",
    "            df_plot=df_plot[['state','COUNTRY','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['COUNTRY','date']).agg(np.sum).reset_index()\n",
    "       #print(show_doubling)\n",
    "\n",
    "\n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                                y=df_plot[show_doubling],\n",
    "                                mode='markers+lines',\n",
    "                                opacity=0.9,\n",
    "                                name=each\n",
    "                        )\n",
    "                )\n",
    "\n",
    "    return {\n",
    "            'data': traces,\n",
    "            'layout': dict (\n",
    "                height=900,\n",
    "\n",
    "                xaxis={'title':'Timeline',\n",
    "                        'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(size=14,color=\"#7f7f7f\"),\n",
    "                      },\n",
    "\n",
    "                yaxis=my_yaxis\n",
    "        ) \n",
    "    }\n",
    "\n",
    "@app.callback(\n",
    "    Output('SIR_model', 'figure'),\n",
    "    [Input('country_dropdown_sir', 'value')])\n",
    "\n",
    "def SIR_fig(con_input):\n",
    "    df= df_input_sir\n",
    "   \n",
    "    \n",
    "    for i in df[1:]:\n",
    "        data = []\n",
    "        trace = go.Scatter(x=df.date,\n",
    "                        y=df[con_input],\n",
    "                        mode='lines+markers',\n",
    "                        name = con_input)\n",
    "        data.append(trace)\n",
    "        \n",
    "        trace_fitted = go.Scatter(x=df.date,\n",
    "                        y=df[con_input +'_fitted'], \n",
    "                        mode='lines+markers',\n",
    "                        name=con_input+'_fitted')\n",
    "        data.append(trace_fitted)\n",
    "        \n",
    "        \n",
    "            \n",
    "    return {'data': data,\n",
    "            'layout' : dict(\n",
    "                height=900,\n",
    "                title= 'SIR model',\n",
    "                xaxis={'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(size=14,color=\"#7f7f7f\"),\n",
    "                      },\n",
    "                yaxis={'type':\"log\",\n",
    "                       'range':'[1.1,5.5]'\n",
    "                      }\n",
    "                \n",
    "            )\n",
    "        }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    app.run_server(debug=True, port= 8051, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
